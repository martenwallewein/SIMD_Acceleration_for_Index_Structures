\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{multicol}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{SIMD Acceleration for Index Structures\\
}

\author{\IEEEauthorblockN{Marten Wallewein-Eising}
\IEEEauthorblockA{\textit{Otto-von-Guericke University} \\
Magdeburg, Germany \\
marten.wallewein-eising@st.ovgu.de}
}

\maketitle

\begin{abstract}
\begin{itemize}
	\item summary: 
	\subitem Give short an overview of SIMD and modern index structures
	\subitem Explain what are the problems of the ``old" index structures made for disk-based database systems
	\subitem Explain which approaches were made to adapt index structures to modern systems and what they have in common and what are differences
	\item Why is this work important: 
	\subitem Give a state of current development of the index structures
	\subitem Collect common approaches to adapt other index structures TODO: ReThink
	\item K-ary search trees, FAST, VAST and ART compared
	\item Contribution: What are important approaches used by different implementations to adapt index structures to modern systems
\end{itemize}
\end{abstract}

\begin{IEEEkeywords}
SIMD, index 
\end{IEEEkeywords}

\section{Introduction}
% Main Problem: Index structures designed for disk-based database systems
After decades of creating and improving index structures for disk-based database systems, nowadays even large databases fit into the main memory. Since index structures like the $B^+$-tree [TODO: ref] or the radix tree [TODO: ref] have an important part in database systems to realise scan or range-based search operations, these index structures experienced many adaptions to fulfill the needs of modern database-systems. Instead of overcoming the bottleneck of IO-oprations from disk to RAM, the target of modern index-structures is to improve the usage of CPU cache and processor architectures.

% Introduce tree structures and specific Problem: Cache and TLB misses and branch mispredictions cost a lot of calculation effort
Several index structures have already shown that the bottleneck from RAM to CPU can be overcome using Single Instruction Multiple Data (SIMD) [1] operations. These index structures include: the K-ary Search Tree (Seg-tree) [TODO: ref], Adapted Radix Tree (ART) [4], Fast Architecture Sensitive Tree (FAST) [6], and Vector-Advanced and Compressed Structure Tree (VAST) [5]. As the authors of VAST-Tree show, important causes for increased runtime are cache misses and branch mispredictions. To overcome branch mispredictions and to decrease CPU cycles, SIMD [TODO: ref] is used in modern index structures for tree traversal. The authors of the k-ary search show how to use SIMD to compare mutliple keys in one CPU cycle. To decrease cache misses, the authors of FAST and ART show how to adapt index structures to the cache line size.  

% Objectives and Contribution
%TODO: Rewrite, all approaches use SIMD but in different ways, therefore difficult to compare...
All approaches use SIMD only for key comparison within tree traversal and try to decrease the key size to fit more keys into one SIMD register. Therefore FAST and Seg-tree only provide implementations for search algorithms. We consider the design approaches VAST and ART make to implement operations like update and insert und name ideas to use SIMD for them. Consequently, with this work we make the following contributions:
\begin{itemize}
	\item We compare different adaptions of index structures to fulfill requirements of modern database systems
	\item We highlight the usage of SIMD and the cache line adaptions in all approaches
	\item We show opportunities for adaption for other approaches to use SIMD
\end{itemize}
% Paper structure
We organized the rest of the paper as follows. In Section 2 we give the preliminaries for SIMD in general and for the use in index structures. In Section 3 we analyse the different approaches of adapted index structures and evaluate the comparision in Section 4. In section 5 we name related work. In Section 6 we present our conclusion und describe future work in Section 7. 
%\begin{figure*}
%	\includegraphics[width=\textwidth]{figure_1.png}
%	\caption{Example of a figure caption.}
%	\label{fig}
%\end{figure*}
\section{Preliminaries}
%TODO: Figure for SIMD, pro/contra's
% Horizontal vs vertical vector processing
A common approach of decreasing CPU cycles for algorithms is to adapt the algorithm to pipelining. While one instruction is executed, the next instruction is already fetched. In contrast to execute one operation on one data item after another, the idea of SIMD is to execute a single instruction on multiple data. Polychroniou et al. [TODO: ref] show two general approaches to use SIMD in in-memory databases, horizontal and vertical vector processing. They name the comparision of one search key to multiple other keys horizontal vectorization, whereas processing a different input key per vector lane is named vertical vectorization. TODO: Exact difference?

% How horizontal vectorization works
Since FAST, Seg-Tree, ART and VAST only use horizontal vectorization, we focus on this approach. Modern CPUs have additional SIMD registers along with an additional instruction set adapted to process multiple data with a single instruction. For example, the authors of Seg-Tree [3] use 128-bit SIMD registers and adjusted SIMD operations to load data into a register and to compare the data of one SIMD register with another. A 128-bit SIMD register processes sixteen 8-bit or eight 16-bit data items with one instruction. In Table 1 TODO: Insert Table! we show a comparision of key size and the number of keys that can be processed parallel with one SIMD instruction.

% SIMD restrictions
The main restriction of SIMD instructions is that a sequential load of data is required. To load data into a SIMD registert, the data has to be stored consecutively in main memory.
%TODO: Figure over 2 columns, include lipsum/multicol packe 
\section{Adapted Tree Structures}
% TODO: Compare all 4 or merge FAST and VAST together/ extend FAST with VAST??
\subsection{Seg-Tree}\label{SCM}
% Basics k-ary search
Zeuch et al. [TODO: ref] adapted the $B^+$-Tree by having a k-ary search tree as each inner node, called segment, and perform a k-ary search on each segment. The k-ary search bases on the binary search but divides the search space into k partitions with k-1 seperators. Compared to binary search the k-ary search reduces the complexity from $O(\log{2}{n})$ to $O(\log{k}{n})$. They consider $m$ as the most bits to represent a datatype and $\vert SIMD \vert$ as the size of SIMD-register, called SIMD bandwith. Furthermore, $k = \frac{\vert SIMD \vert }{m}$ defines the count of partitions for the k-ary search. 

% Performing k-ary search on Seg-tree
As mentioned before, each segment of the Seg-Tree is a k-ary search tree. To perform a k-ary search on a segment, Zeuch et al. [TODO: ref] make a linearization of the segment. They show two algorithms for linearization, breadth-first search and depth-first search. Because of the condition $k = \frac{\vert SIMD \vert }{m}$, each partition of the k-ary search fits into a SIMD register and can be compared to the search key. A perfect k-ary search tree contains  $S_{max} = k^h - 1$ keys for an integer $h > 0$. The considered search algorithm only works for sequences with a multiple of $k-1$ keys. In case of  sequences with less than a multiple of $k-1$ keys, they replenish the sequence with Elements having the value $k_{max} + 1$ for the maximal key value $k_{max}$ in the sequence. Consequently, the adapted search algorithm works for sequences with less than a multiple of $k-1$ keys.
\textbf{TODO: Fix: In figure 1, we show the adaption of nodes made by the authors of Seg-Tree. Todo: Figure!}

% Performance increase
The performance of Seg-Tree depends on k-ary search. The smaller a key the more keys are compared parallel. According to the relevance of 32 and 64-bit data types in modern systems, the k-ary search performance increases only by the factor of four for 32-bit types and two for 64-bit types.

% Addional adaption: Seg-trie
%TODO: Each Node is also k-ary search tree
Zeuch et al. [TODO: ref] also show the k-ary search on an adapted prefix tree \emph{(trie for short)} [TODO: ref] called Seg-Trie. A trie is a search tree where each node stores a part of the key. Each node is again designed as a k-ary search tree. TODO: Useful? Complete keys are stored in leave nodes or are build by concatenating partial keys from the root node to a leave node. This approach benefits of the separation of the keys in different levels of the tree. Consequently, the compare keys are smaller and more keys can be compared parallel. The Seg-Trie$_L$ is defined as a balanced trie where each node on each level contains one part of the key with $L$ Bits. The tree has $r = \frac{m}{L}$ levels $(E_0, E_1, .., E_r)$, where m is the number of most bits to represent the data type.

% K-ary search on Seg-Trie
To perfom a tree traversal on the seg-trie, the search key is split into $r$ segments and each segment $r_i$ is compared to the level $E_i$. If a matching partial key is found in one node of $E_i$, the search continues at the referrenced node for the partial key. If no match of the partial key is found, the Seg-Trie does not contain the search key and the search is finished. Consequently, the advantage of Seg-Trie against tree structures is the reduced comparision effort from non-exisiting key segments. 

% Performance increase of Seg-Trie

\subsection{FAST}\label{SCM}
Changkyu et al. adapted a binary tree to optimize for architecture features like page size, cache line size, and SIMD bandwith. They show the performance increase because of decreasing cache misses and better cache line usage.  
\subsection{ART}\label{SCM}

\subsection{VAST}\label{SCM}

\section{Evaluation}
In common:
\begin{itemize}
	\item SIMD instructions used to compare the search key with multiple keys of the index
	\item Segmenting tree to blocks for a better usage of cache lines, save the data of the nodes in an adapted way
	\item The keys should be as short as possible to compare more keys in one step and to decrease the passed data to the cache line
	\item Each approach improves the tree traversal
\end{itemize}

Differences:
\begin{itemize}
	\item Node compression in VAST, Path compression in ART and K-ary seg trie
	\item FAST and K-ary trees readonly to improve traversal, ART and FAST adapt insert too
	\item FAST uses and K-ary trees will use GPU calculation instead of CPU
\end{itemize}

Why performance can not be compared in a useful way...
performance issues:
\begin{itemize}
	\item Seg-Tree: 
\end{itemize}
\section{Related Work}
TODO:
\begin{itemize}
	\item ART and VAST compared to FAST??
	\item Ideas and implementations of the adapted trees already in III...
	\item KD-Tree with SIMD
\end{itemize}

\section{Conclusion}

\section{Future work}
Open questions, use SIMD for tree creation/updates instead of only for traversal
\begin{thebibliography}{00}
\bibitem{b1}Mohammad Suaib, Abel Palaty and Kumar Sambhav Pandey, ``Architecture of SIMD Type Vector Processor'' in International Journal of Computer Applications (0975 - 8887) Volume 20 No.4, April 2011.
\bibitem{b2} Jingren Zhou and Kenneth A. Ross  ``Implementing Database Operations Using SIMD Instructions'' in ACM S1GMOD '2002 June 4-6, Madison, Wisconsin, USA
\bibitem{b3} Steffen Zeuch, Frank Huber and Johann-Christoph Freytag  ``Adapting Tree Structures for Processing with SIMD Instructions'' in Proc. 17th International Conference on Extending Database Technology (EDBT), March 24-28, 2014, Athens, Greece
\bibitem{b4} Viktor Leis, Alfons Kemper and Thomas Neumann ``The Adaptive Radix Tree: ARTful Indexing for Main-Memory Databases''
\bibitem{b5} Takeshi Yamamuro, Makoto Onizuka,Toshio Hitaka, and Masashi Yamamuro ``VAST-Tree: A Vector-Advanced and Compressed Structure for Massive Data Tree Traversal'' in EDBT 2012, March 26-30, 2012, Berlin, Germany.
\bibitem{b6} Changkyu Kim, Jatin Chhugani, Nadathur Satish, Eric Sedlar, Anthony D. Nguyen,
Tim Kaldewey, Victor W. Lee, Scott A. Brandt and Pradeep Dubey ``FAST: Fast Architecture Sensitive Tree Search
on Modern CPUs and GPUs'' in SIGMOD’10, June 6-11, 2010, Indianapolis, Indiana, USA.
\end{thebibliography}

\end{document}
